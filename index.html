<!DOCTYPE html>
<html>
  <head>
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-VQTBKP87MK"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-VQTBKP87MK");
    </script>

    <meta charset="utf-8" />
    <meta
      name="description"
      content="Self-Guided Action Diffusion"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Self-Guided Action Diffusion
    </title>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
      google.load("jquery", "1.3.2");
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$', '$']]
          },
          "HTML-CSS": {
          availableFonts: ["TeX", "STIX-Web", "Asana-Math", "Latin-Modern"], // Specify the desired font here
          preferredFont: "STIX-Web", // Set the preferred font
          webFont: "STIX-Web" // Set the web font to use
          },
      });
    </script>
    <script
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
      type="text/javascript"
    ></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Self-Guided Action Diffusion
              </h1>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Rhea Malhotra<sup>1</sup>,
                </span>
                <span class="author-block">
                  Yuejiang Liu<sup>1</sup>,
                </span>
                <span class="author-block">
                  Chelsea Finn<sup>1</sup>
                </span>
              </div>
              

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Stanford University
                </span>
              </div>

              <br />
              <img src="./media/figures/stanford.png" width="15%" />

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      href="https://github.com/rhea-mal"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Code Link. -->
                  <span class="link-block">
                    <a
                      href="https://github.com/rhea-mal"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fa-brands fa-github"></i>
                      </span>
                      <span>LeRobot Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a
                      href="https://github.com/rhea-mal"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fa-brands fa-github"></i>
                      </span>
                      <span>N1 Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Title Card + Caption -->
    <section class="hero teaser">
      <div class="container is-fullhd">
        <div class="hero-body">
          <div class="container">
            <div class="columns is-vcentered  is-centered">
              <img 
            src="media/figures/adaptive_new.png" 
            alt="Self-Guided Action Diffusion Figure"
            style="max-width: 100%; height: auto; display: block; margin: auto;" />

            </div>
            <br />
            <h2 class="subtitle has-text-centered">
              <span class="dmtpi">Self-Guided Action Diffusion</span>
              allows for inference-time adaptation to temporally coherence noise in an environment for higher performance in diffusion and flow-matching policies.
            </h2>
          </div>
        </div>
      </div>
    </section>

    <hr />

    <!-- Abstract. -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Recent works have shown the promise of inference-time search over action samples for improving generative robot policies. In particular, optimizing cross-chunk
                coherence via bidirectional decoding has proven effective in boosting the consistency and reactivity of diffusion policies. However, this approach remains computationally expensive as the diversity of sampled actions grows. In this paper, we
                introduce self-guided action diffusion, a more efficient variant of bidirectional decoding tailored for diffusion-based policies. At the core of our method is to guide
                the proposal distribution at each diffusion step based on the prior decision. Experiments in simulation tasks show that the proposed self-guidance enables near-
                optimal performance at negligible inference cost. Notably, under a tight sampling
                budget, our method achieves up to 70% higher success rates than existing counterparts on challenging dynamic tasks.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>

    </section>

    <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Key Findings</h2>
          <ul class="content is-medium">
            <li><strong>High Task Success:</strong> Self-Guided Action Diffusion improves Robomimic benchmarks success rates by <strong>~70%</strong> over diffusion policy baselines in closed loop settings.</li>
            <li><strong>Generalization:</strong> On the DexMimicGen Cross-Embodiment Suite and RoboCasa, our method outperforms the GROOT-N1 foundation model by  <strong>28%</strong> and <strong>12%</strong> respectively.</li>
            <li><strong>Robustness:</strong> Our method maintains high performance even in stochastic environments and across diverse demonstration datasets.</li>
            <li><strong>Sample Efficiency:</strong> Compared to coherence sampling, Self-GAD achieves higher success with fewer samples by leveraging prior gradients at inference-time.</li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  <!-- APPROACH -->

  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="content has-text-justified">
          <p>
            Self-Guided Action Diffusion (Self-GAD) enhances diffusion-based robot policies by injecting a 
            guidance signal during inference. At each denoising step, we apply a lightweight update that encourages 
            actions to stay close to prior predictions while allowing adaptation:
          </p>
          <div style="text-align: center;">
            <script type="math/tex">
              (\hat{s}_t, \hat{a}_t) \leftarrow (\hat{s}_t, \hat{a}_t) + \beta \nabla_{(\hat{s}_t, \hat{a}_t)} \mathcal{L}
            </script>
          </div>
          <p>
            The loss penalizes deviations from the prior trajectory with an exponentially decaying weight:
          </p>
          <div style="text-align: center;">
            <script type="math/tex">
              \mathcal{L} = \sum_{i = t + h}^{t + l} 0.5^{i - (t + h)} \cdot \left\| \hat{a}_i - a_i^{\text{prior}} \right\|_2^2
            </script>
          </div>
          <p>
            Further, we integrate Self-GAD into GR00T-N1, a robotic foundation model rooted in a flow-matching Diffusion Transformer. This plug-in method improves closed-loop performance, 
            particularly under persistent noise and shifting targets that challenge standard open-loop diffusion policies.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- EXPERIMENTS -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiments</h2>
    <div class="content has-text-justified">

      <h3 class="title is-4">How does Self-GAD perform in single-sample closed-loop settings?</h3>
      <p>
        We evaluate Self-GAD against baseline diffusion policies using a single-sample closed-loop control strategy. 
        Self-GAD outperforms Random sampling, achieving an average success rate 71.4% higher across all Robomimic benchmarks.
      </p>
      <figure class="image">
        <img src="media/figures/final.png" alt="Final Policy Comparison" />
        <figcaption class="has-text-centered">
          Comparison of Sampling Methods in Single-Sample Settings
        </figcaption>
      </figure>
      <br />

      <h3 class="title is-4">How does Self-GAD scale to foundation models like GR00T-N1-2B?</h3>
      <p>
        We integrate Self-GAD into GR00T-N1-2B, a large-scale robotic foundation model using flow-matching and multimodal embeddings. 
        We fine-tune GR00T-N1-2B on 100 demonstrations per task in single action horizon settings (PnP Counter to Cab, Turn Stove On, Turn Sink Faucet On, Turn Microwave Off, Coffee, and Transport). \methodacro{} integrated with the robotic foundation models boosts success rates by in both RoboCasa and DexMG, by 28.4% and 12% respectively.
      </p>
      <figure class="image">
        <img src="media/figures/N1.png" alt="GR00T-N1 Results" />
        <figcaption class="has-text-centered">
          Self-GAD in a Generalized Robotic Foundation Model
        </figcaption>
      </figure>
      <br />

      <h3 class="title is-4">How does Self-GAD improve sample efficiency?</h3>
      <p>
        Compared to coherence sampling, Self-GAD achieves high performance with fewer samples. Here, Self-GAD achieves near-optimal performance with a single sample, maintained from 16 samples in PushT.
      </p>
      <figure class="image">
        <img src="media/figures/sampleefficiency.png" alt="Sample Efficiency" />
        <figcaption class="has-text-centered">
          Sample Efficiency
        </figcaption>
      </figure>
      <br />

      <h3 class="title is-4">How robust is Self-GAD to stochasticity and diverse demonstrations?</h3>
      <p>
        We evaluate Self-GAD in noisy environments and across varied training datasets. The method consistently generalizes under distribution shift and environmental perturbations. On the RoboMimic Square task, guidance improves consistency in single-sample rollouts, with benefits increasing in high-variance settings as action diversity grows. In dynamic PushT environments, Self-GAD significantly boosts closed-loop performance, particularly under high variability.
      </p>

      <div class="columns is-centered is-vcentered">
        <div class="column is-half">
          <figure class="image" style="height: 300px; display: flex; align-items: center; justify-content: center;">
            <img src="media/figures/datasetvariance.png" alt="Dataset Variance" style="height: 100%; object-fit: contain;" />
          </figure>
          <figcaption class="has-text-centered">
            Guidance Enhances Robustness to Dataset Variance
          </figcaption>
        </div>

        <div class="column is-half">
          <figure class="image" style="height: 300px; display: flex; align-items: center; justify-content: center;">
            <img src="media/figures/dynamic.png" alt="Dynamic Environment Robustness" style="height: 100%; object-fit: contain;" />
          </figure>
          <figcaption class="has-text-centered">
            Self-GAD in Dynamic Settings Across Action Horizons
          </figcaption>
        </div>
      </div>



      <h3 class="title is-4">How does Self-GAD weight past predictions during inference?</h3>
      <p>
        Self-GAD reduces the performance gap between static and dynamic environments. Here, we confirm the reusability of a finetuned beta weight for prior relevance across dynamic settings.
      </p>
      <figure class="image">
        <img src="media/figures/dynamic_weights.png" alt="Temporal Weighting" />
        <figcaption class="has-text-centered">
          Temporal weighting under Dynamic Conditions
        </figcaption>
      </figure>

    </div>
  </div>
</section>




      

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content has-text-centered">
              <p>
                Website template borrowed from
                <a href="https://github.com/nerfies/nerfies.github.io"
                  >NeRFies</a
                >
                and <a href="https://peract.github.io/">PerAct</a> and
                <a href="https://voxposer.github.io/">VoxPoser</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
